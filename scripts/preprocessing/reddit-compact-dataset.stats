CREATE TEMPLATE DATASETS
date_week, 500
=============== Politics ===============
Reading data with header: ['author', 'full_link', 'num_comments', 'score', 'text', 'created_datetime', 'date_hour', 'date_day', 'date_week', 'date_month', 'author_code']
Reading documents: data/raw/reddit/politics/submissions/aggregated/data_0_comments.csv: 100%|#########9| 36125/36129 [10:15<00:00, 58.72document/s]
Sorting Documents
SENTENCE:
         tokenization and preprocessing ...
Tokenize documents for GRU/LSTM LM: 100%|##########| 35004/35004 [00:55<00:00, 631.99it/s]
Transformers Document Tokenization: 100%|##########| 35004/35004 [00:00<00:00, 38423.05it/s]
         counting document frequency of words ...
         building the vocabulary sentence ...
         vocabulary size: 12483
BOW:
         removing punctuation ...
100%|##########| 35004/35004 [01:37<00:00, 358.73it/s]
         counting document frequency of words ...
         vocabulary size after removing stopwords from list: 5000
tokenizing documents and splitting into train/test/valid/prediction...
total number of time steps: 144
total number of train time steps: 124
  removing words from vocabulary not in training set ...
         bow vocabulary after removing words not in train: 5000
         sentence vocabulary after removing words not in train: 12473
Train Size 25432
Validation Size 3179
Test Size 3179
Prediction Size 3214
 total vocabulary size: 12629
  create dictionary and inverse dictionary
tokenizing bow ...
tokenizing sentences ...
removing empty documents ...
splitting test documents in 2 halves...
creating lists of words...
  len(words_tr):  453162
  len(words_va):  57404
  len(words_va):  58243
  len(words_te_h1):  28357
  len(words_te_h2):  29886
  len(words_pr):  56519
getting doc indices...
  len(np.unique(doc_indices_tr)): 25432 [this should be 25432]
  len(np.unique(doc_indices_va)): 3179 [this should be 3179]
  len(np.unique(doc_indices_te)): 3179 [this should be 3179]
  len(np.unique(doc_indices_te_h1)): 3179 [this should be 3179]
  len(np.unique(doc_indices_te_h2)): 3179 [this should be 3179]
  len(np.unique(doc_indices_pr)): 3214 [this should be 3214]
creating bow representation...
bow => tf idf
padding training dataset
exporting training dataset...
padding validation dataset
exporting validation dataset...
padding test dataset
exporting test dataset...
padding prediction dataset
export prediction dataset...
counting words ...
load embeddings...
=============== Donald ===============
Reading data with header: ['author', 'full_link', 'num_comments', 'score', 'text', 'created_datetime', 'date_hour', 'date_day', 'date_week', 'date_month', 'author_code']
Reading documents: data/raw/reddit/the_donald/submissions/aggregated/data_0_comments.csv:  99%|#########8| 143286/144827 [55:06<00:35, 43.34document/s]
Sorting Documents
SENTENCE:
         tokenization and preprocessing ...
Tokenize documents for GRU/LSTM LM: 100%|##########| 56425/56425 [01:49<00:00, 514.40it/s]
Transformers Document Tokenization: 100%|##########| 56425/56425 [00:02<00:00, 23210.61it/s]
         counting document frequency of words ...
         building the vocabulary sentence ...
         vocabulary size: 22493
BOW:
         removing punctuation ...
100%|##########| 56425/56425 [03:35<00:00, 261.75it/s]
         counting document frequency of words ...
         vocabulary size after removing stopwords from list: 5000
tokenizing documents and splitting into train/test/valid/prediction...
total number of time steps: 113
total number of train time steps: 93
  removing words from vocabulary not in training set ...
         bow vocabulary after removing words not in train: 4996
         sentence vocabulary after removing words not in train: 22380
Train Size 37140
Validation Size 4643
Test Size 4642
Prediction Size 10000
 total vocabulary size: 22418
  create dictionary and inverse dictionary
tokenizing bow ...
tokenizing sentences ...
removing empty documents ...
splitting test documents in 2 halves...
creating lists of words...
  len(words_tr):  1240577
  len(words_va):  159242
  len(words_va):  154933
  len(words_te_h1):  76309
  len(words_te_h2):  78624
  len(words_pr):  295006
getting doc indices...
  len(np.unique(doc_indices_tr)): 37140 [this should be 37140]
  len(np.unique(doc_indices_va)): 4643 [this should be 4643]
  len(np.unique(doc_indices_te)): 4642 [this should be 4642]
  len(np.unique(doc_indices_te_h1)): 4642 [this should be 4642]
  len(np.unique(doc_indices_te_h2)): 4642 [this should be 4642]
  len(np.unique(doc_indices_pr)): 10000 [this should be 10000]
creating bow representation...
bow => tf idf
padding training dataset
exporting training dataset...
padding validation dataset
exporting validation dataset...
padding test dataset
exporting test dataset...
padding prediction dataset
export prediction dataset...
counting words ...
load embeddings...
=============== Wallstreetbets ===============
Reading data with header: ['author', 'full_link', 'num_comments', 'score', 'text', 'created_datetime', 'date_hour', 'date_day', 'date_week', 'date_month', 'author_code']
Reading documents: data/raw/reddit/wallstreetbets/submissions/aggregated/data_0_comments.csv:  98%|#########7| 143053/146196 [1:21:04<01:46, 29.41document/s]
Sorting Documents
SENTENCE:
         tokenization and preprocessing ...
Tokenize documents for GRU/LSTM LM: 100%|##########| 45254/45254 [02:20<00:00, 322.40it/s]
Transformers Document Tokenization: 100%|##########| 45254/45254 [00:04<00:00, 10325.32it/s]
         counting document frequency of words ...
         building the vocabulary sentence ...
         vocabulary size: 23762
BOW:
         removing punctuation ...
100%|##########| 45254/45254 [04:54<00:00, 153.84it/s]
         counting document frequency of words ...
         vocabulary size after removing stopwords from list: 5000
tokenizing documents and splitting into train/test/valid/prediction...
total number of time steps: 92
total number of train time steps: 72
  removing words from vocabulary not in training set ...
         bow vocabulary after removing words not in train: 4998
         sentence vocabulary after removing words not in train: 23438
Train Size 28297
Validation Size 3538
Test Size 3537
Prediction Size 9882
 total vocabulary size: 23481
  create dictionary and inverse dictionary
tokenizing bow ...
tokenizing sentences ...
removing empty documents ...
splitting test documents in 2 halves...
creating lists of words...
  len(words_tr):  1712085
  len(words_va):  221654
  len(words_va):  211067
  len(words_te_h1):  104642
  len(words_te_h2):  106425
  len(words_pr):  802120
getting doc indices...
  len(np.unique(doc_indices_tr)): 28297 [this should be 28297]
  len(np.unique(doc_indices_va)): 3538 [this should be 3538]
  len(np.unique(doc_indices_te)): 3537 [this should be 3537]
  len(np.unique(doc_indices_te_h1)): 3537 [this should be 3537]
  len(np.unique(doc_indices_te_h2)): 3537 [this should be 3537]
  len(np.unique(doc_indices_pr)): 9882 [this should be 9882]
creating bow representation...
bow => tf idf
padding training dataset
exporting training dataset...
padding validation dataset
exporting validation dataset...
padding test dataset
exporting test dataset...
padding prediction dataset
export prediction dataset...
counting words ...
load embeddings...
=============== Science ===============
Reading data with header: ['author', 'full_link', 'num_comments', 'score', 'text', 'created_datetime', 'date_hour', 'date_day', 'date_week', 'date_month', 'author_code']
Reading documents: data/raw/reddit/askscience/submissions/aggregated/data_0_comments.csv: 100%|##########| 26621/26621 [07:07<00:00, 62.25document/s]
Sorting Documents
SENTENCE:
         tokenization and preprocessing ...
Tokenize documents for GRU/LSTM LM: 100%|##########| 26291/26291 [00:41<00:00, 640.12it/s]
Transformers Document Tokenization: 100%|##########| 26291/26291 [00:00<00:00, 43818.04it/s]
         counting document frequency of words ...
         building the vocabulary sentence ...
         vocabulary size: 9318
BOW:
         removing punctuation ...
100%|##########| 26291/26291 [01:13<00:00, 359.43it/s]
         counting document frequency of words ...
         vocabulary size after removing stopwords from list: 5000
tokenizing documents and splitting into train/test/valid/prediction...
total number of time steps: 45
total number of train time steps: 25
  removing words from vocabulary not in training set ...
         bow vocabulary after removing words not in train: 4968
         sentence vocabulary after removing words not in train: 9108
Train Size 11259
Validation Size 1408
Test Size 1407
Prediction Size 12217
 total vocabulary size: 9413
  create dictionary and inverse dictionary
tokenizing bow ...
tokenizing sentences ...
removing empty documents ...
splitting test documents in 2 halves...
creating lists of words...
  len(words_tr):  180026
  len(words_va):  22527
  len(words_va):  22444
  len(words_te_h1):  10867
  len(words_te_h2):  11577
  len(words_pr):  194018
getting doc indices...
  len(np.unique(doc_indices_tr)): 11259 [this should be 11259]
  len(np.unique(doc_indices_va)): 1408 [this should be 1408]
  len(np.unique(doc_indices_te)): 1407 [this should be 1407]
  len(np.unique(doc_indices_te_h1)): 1407 [this should be 1407]
  len(np.unique(doc_indices_te_h2)): 1407 [this should be 1407]
  len(np.unique(doc_indices_pr)): 12217 [this should be 12217]
creating bow representation...
bow => tf idf
padding training dataset
exporting training dataset...
padding validation dataset
exporting validation dataset...
padding test dataset
exporting test dataset...
padding prediction dataset
export prediction dataset...
counting words ...
load embeddings...